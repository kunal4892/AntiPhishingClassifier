{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti Phishing Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Metrics for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset and X_train Y_train and X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_url = 'datasets/urlset.csv'\n",
    "# data = pd.read_csv(dataset_url)\n",
    "# data = shuffle(data).reset_index(drop=True)\n",
    "# #data.shape\n",
    "# labels = data.iloc[:, lambda df: [-1]]\n",
    "# data = data.loc[:,lambda df:['domain', 'ranking']]\n",
    "# #labels.shape\n",
    "# #X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "# #X_train.shape\n",
    "# #X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Feature set from already produced features using .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = '../Featureset/combined_dataset.csv'\n",
    "data = pd.read_csv(featureset)\n",
    "sscaler = StandardScaler()\n",
    "#data = sscaler.fit_transform(data)\n",
    "labels = data.iloc[:, lambda df: [-1]]\n",
    "data = data.iloc[:,2:-2]\n",
    "\n",
    "train_data = data.iloc[0:76000,:]\n",
    "labels_train = labels.iloc[0:76000,:]\n",
    "\n",
    "test_data = data.iloc[76001:,:]\n",
    "labels_test = labels.iloc[76000:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features is used to create features for the classification:\n",
    "'''\n",
    "1. Domain\n",
    "2. URL_length\n",
    "3. URL_slashes_count\n",
    "4. URL_dots_count\n",
    "5. URL_hostname_len\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data_frame):\n",
    "    url = data_frame['domain']\n",
    "    len_url = []\n",
    "    for u in url:\n",
    "        len_url.append(len(u))\n",
    "\n",
    "    data_frame['URL_length'] = pd.Series(len_url)\n",
    " \n",
    "    count_slashes = []\n",
    "    for u in url:\n",
    "        count_slashes.append(u.count('//') )\n",
    "    data_frame['URL_slashes_count'] = pd.Series(count_slashes)\n",
    "    \n",
    "    count_dots = []\n",
    "    for u in url:\n",
    "        count_dots.append(u.count('.') )\n",
    "    data_frame['URL_dots_count'] = pd.Series(count_dots)\n",
    "\n",
    "    len_hostname = []\n",
    "    start = '://'\n",
    "    end = '/'\n",
    "\n",
    "    for u in url:\n",
    "        temp = u[u.find(start)+2*len(start) + 2: u.rfind(end)]\n",
    "        temp = temp.replace('/','.')\n",
    "        temp = temp.replace('-','.')\n",
    "        len_hostname.append(len(temp.split('.')))\n",
    "\n",
    "    data_frame['URL_hostname_len'] = pd.Series(len_hostname)\n",
    "    data_frame = data_frame.iloc[:,1:]\n",
    "    return data_frame\n",
    "#     total_word_count = 5000\n",
    "#     tokenizer = Tokenizer(num_words=total_word_count)\n",
    "#     tokenizer.fit_on_texts(clean_url)\n",
    "\n",
    "#     seq_length = 5 #Number of items in each sequence\n",
    "#     sequences = tokenizer.texts_to_sequences(clean_url)\n",
    "#     data = pad_sequences(sequences, maxlen=seq_length)\n",
    "\n",
    "#     num_data = phish_data[['create_age(months)', 'expiry_age(months)', 'update_age(days)', 'URL_length', 'URL_slashes', 'URL_dots', 'URL_host']].values\n",
    "#     num_lab = phish_data[\"Label\"].values\n",
    "\n",
    "#     sscaler = StandardScaler()\n",
    "#     num_data_scaled = sscaler.fit_transform(num_data)\n",
    "#     num_data = num_data_scaled\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify classifies the data in different using all the different  classifiers.\n",
    "\n",
    "'''\n",
    "1. Log-Regression\n",
    "2. K-Nearest Neighbors - @params knn neighbors\n",
    "3. Decision Tree - @params maximum depth\n",
    "4. Random Forest - @params maximum depth, criterion, max_features, num_of_estimators\n",
    "5. AdaBoost\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_clf_after_CrossVal(train_x, train_y, classifier_parameter_map):\n",
    "    scoring_evals = {'AUC': 'roc_auc',\n",
    "                     'Accuracy': make_scorer(accuracy_score),\n",
    "                     'f1': make_scorer(f1_score)\n",
    "                    }\n",
    "    clf_list=[]\n",
    "    for key in classifier_parameter_map:\n",
    "        clf = GridSearchCV(classifier_parameter_map[key][0],\n",
    "                     classifier_parameter_map[key][1],\n",
    "                     #iid=False,\n",
    "                     scoring = 'accuracy',\n",
    "                     #refit = 'Accuracy',\n",
    "                     cv=10, # no of validations\n",
    "                     n_jobs = -1 # use full concurrency\n",
    "                    )\n",
    "        \n",
    "        result = clf.fit(train_x, train_y)\n",
    "        #print(clf.cv_results_)\n",
    "        print(result.best_estimator_)\n",
    "        print('score: ', result.best_score_)\n",
    "        clf_list.append(result.best_estimator_)\n",
    "    return clf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify(train_x, train_y):\n",
    "           \n",
    "    logreg_parameters = {\n",
    "     'penalty': ['l1','l2']\n",
    "    }\n",
    "    \n",
    "    knn_parameters = {\n",
    "       'n_neighbors': np.arange(2,100,1),\n",
    "       'weights': ['distance'],\n",
    "       'metric': ['minkowski']\n",
    "    }\n",
    "    dt_parameters = {\n",
    "        'min_samples_split' : range(2,50,2),\n",
    "        #'max_depth': range(1,50,2)\n",
    "    }\n",
    "    \n",
    "    rf_parameters = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': range(1,20,1),\n",
    "    'n_estimators': range(2,100,1)\n",
    "    }\n",
    "    \n",
    "    classifier_parameter_map = {\"Log-Regression\": (LogisticRegression(), logreg_parameters),\n",
    "                          \"K-Nearest Neighbors\": (KNeighborsClassifier(), knn_parameters),\n",
    "                          \"Decision Tree\": (DecisionTreeClassifier(), dt_parameters),\n",
    "                          \"Random Forest\": (RandomForestClassifier(), rf_parameters)}\n",
    "                                            #\"AdaBoost\"}\n",
    "        \n",
    "    clf_list = get_best_clf_after_CrossVal(train_x, train_y, classifier_parameter_map)\n",
    "    print(len(clf_list))\n",
    "    return clf_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kumar4892/projects/fml/py3venv/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "score:  0.8726842105263158\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=22, p=2,\n",
      "                     weights='distance')\n",
      "score:  0.9416973684210527\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "score:  0.9352763157894737\n"
     ]
    }
   ],
   "source": [
    "clf_list = classify(train_data, np.ravel(labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['Log-Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest']\n",
    "for clfname, clf in zip(classifiers, clf_list):\n",
    "    filename = str('../Model_dump/') + str(clfname) + (str('.sav'))\n",
    "    pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for clfname, clf in zip(classifiers, clf_list):\n",
    "        labels_pred = clf.predict(test_data)\n",
    "    accuracies.append(accuracy_score(labels_test, labels_pred))\n",
    "    print(clfname)\n",
    "    print('Accuracy: ', accuracy_score(labels_test, labels_pred))\n",
    "    print('Classification report')\n",
    "    print(classification_report(labels_test, labels_pred, target_names=['Spam','Legitimate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(classifiers, accuracies,'ro',markersize=12)\n",
    "plt.plot(classifiers, accuracies,color = 'blue', linestyle = 'dashed',linewidth=2, markersize=12)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
